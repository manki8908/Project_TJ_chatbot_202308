{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230614\n",
    "* 챗봇 시스템\n",
    "  * 화장품쇼핑몰 사용자를 위한 챗봇시스템\n",
    "  * 연구실 구성원을 위한 챗봇시스템\n",
    "  * 컴퓨터용품 AS센터 챗봇시스템\n",
    "  * 사람이 타이핑한 글자를 인식\n",
    "\n",
    "* 프로그램: abc.exe\n",
    "* 프로세스: 프로그램이 동적인 메모리로 실행되어 있는 상태\n",
    "* 소멸자: 특정 인스턴스를 강제로 내릴때"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230615\n",
    "* 데이터랭글링(필터링): 데이터 전처리\n",
    "* ```__dict__```: self, 인스턴스 변수에 접근"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230619\n",
    "* 토크나이징\n",
    "  * 토큰: 문장에서 의미가 있는 가장작은 단위\n",
    "  * 문장을 토큰 단위로 나눔\n",
    "* 형태소 분석기\n",
    "  * 형태소: 언어의 가장작은 단위\n",
    "  * 형태소 분석기: 언어를 각종 품사 단위로 쪼개고, 단어와 품사를 태깅\n",
    "  * 단어를 어떻게 쪼개는냐에 따라 모델의 성능이 달라짐\n",
    "* 한국어 자연어 처리\n",
    "  * KoNLPy\n",
    "    * 오픈소스 라이브러리(konlpy.org/ko/latest)\n",
    "    * 세가지 형태소 분석기 모듈을 지원\n",
    "      * Kkma(꼬꼬마)\n",
    "        * morphs(형태소단위), nouns(명사단위), pos(형태소,품사태깅), sentences(여러문장을 분리)\n",
    "      * Komoran(코모란)\n",
    "        * 자바로 개발한 한국어 형태소 분석기\n",
    "        * 공백이 포함된 형태소 단위로도 분석이 가능\n",
    "        * 사전관리하는 방법이 편리하고, 성능과 속도가 괜찮은 편\n",
    "        * morphs(형태소단위), nouns(명사단위), pos(형태소,품사태깅)\n",
    "      * Okt\n",
    "        * 트위터에서 개발한 Twitter 한국어 처리기에서 파생된 오픈소스\n",
    "        * 간단한 한국어 처리를 통해 색인어를 추출하는 목표(형태소 분석X)\n",
    "        * 분석되는 품사 정보는 작지만 속도는 제일 빠름\n",
    "        * normalize 함수를 통해 오타가 섞인 문장을 처리하는데 효과\n",
    "        * morphs, nouns, pos, normalize(정규화, 사랑햌-->사랑해ㅋ), phrases(어구추출)\n",
    "    * 설치방법: https://konlpy.org/ko/latest/install/#id2\n",
    "      * python 뿐만 아니라 java등의 다양한 언어 기반으로 개발되어 여러 의존성이 있음\n",
    "      * python 3.8대에서 호환이 좋음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230620\n",
    "#### 임베딩\n",
    "* 토크나이징 이후\n",
    "* 자연어를 숫자나 벡터 형태변환\n",
    "* 단어나 문장을 수치화해 벡터 공간으로 표현\n",
    "* 임베딩 유형\n",
    "  * 문장 임베딩: 개발 비용이 큼, 챗GPT\n",
    "  * 단어 임베딩: 간다한 챗봇, 동음이의어에 대한 구분을 못하는 단점\n",
    "    * 원핫인코딩: 단어사전의 단위행렬 적용, 유사한 단어와의 관계를 담고 있지 않아 활용도가 낮음.\n",
    "* 단위행렬, 대각행렬\n",
    "  * 대각행렬(diagonal matrix): 정사각 행렬 중 대각선의 값만 존재하는 행렬, 나머지는 0\n",
    "  * 단위행렬(identity matrix): 대각행렬중 주대각 행렬 값이 1인 행렬\n",
    "* 희소표현, 분산표현(밀집표현)\n",
    "  * 희소표현: \n",
    "    * 원핫인코딩\n",
    "    * 직관적\n",
    "    * 단어관 연관성을 담을 수 없음, 차원이 높아지기 쉬우며 이는 모델이 학습을 잘 못하게함(차원의 저주)\n",
    "  * 분산표현\n",
    "    * 임베딩 모델(임베딩을 위한 신경망등의 기법에 기반한 모델): CBOW, skip-gram\n",
    "      * CBOW: 주변 단어를 이용해 타겟 단어를 예측 --> 단순하여 빠름\n",
    "      * skip-gram: 입력 단어를 이용해 주변을 예측 --> 표현력이 더 좋음\n",
    "    * 비 지관적\n",
    "    * 단어간 유사성을 표현하기 위한 노력, 저차원으로 밀집(벡터 공간을 절약), 예) RGB(204,255,204)\n",
    "* word2vec\n",
    "  * \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230621\n",
    "* FAQ, QNA 챗봇 개발 목표\n",
    "* 입력된 질문과, 시스템에 내재된 답변의 유사도를 이용\n",
    "* 단어 수치화\n",
    "  * 라벨인코딩, 원핫인코딩, word2vec(인공신경망)\n",
    "* 유사도\n",
    "  * 단어 또는 문장을 벡터화 한후 통계, 신경망 기법에 따라 유사도 계산\n",
    "  * 통계\n",
    "    * n-gram(문장 유사도)\n",
    "      * 문장을 n개의 토큰으로 분리\n",
    "      * 유사도 = 겹치는 토큰수 / 전체 토큰 수\n",
    "      * 기준 문장이 있어 기준에 따라 유사도 달라짐 \n",
    "      * 논문 도용등 방지 프로그램에 쓰임\n",
    "    * 코사인 유사도\n",
    "      * (1) 두문장의 명사 합집합을 각 문장별 카운트 행렬 생성\n",
    "      * (1)을 입력으로 두 카운트 행렬의 코사인값을 유사도로 생성(-1~1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230622\n",
    "* 텐서플로우와 케라스의 역할 분담\n",
    "    * 텐서플로우\n",
    "      * 구글, 오픈소스\n",
    "      * 기계학습 라이브러리\n",
    "      * python, C++, Java 언어 지원\n",
    "    * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230626~\n",
    "* 6.1 숫자인식 인공신경망 개발\n",
    "  * 이미지 분류\n",
    "  * minist 필기 숫자\n",
    "  * 단순 신경망\n",
    "* 6.2 문장 의도 분류를 위한 CNN \n",
    "  * 목표: 문장 감정 분류\n",
    "  * 원본 데이터: [문장, 감정 레이블]\n",
    "  * 학습자료\n",
    "    * feature 처리 과정\n",
    "      * 1. 문장에서 단어로 분리\n",
    "      * 2. 단어 분리 데이터 라벨 인덱싱\n",
    "      * 3. 인덱싱 데이터 패딩\n",
    "  * 모델 학습\n",
    "    * 임베딩 --> 병렬 CONV1D --> logit dense --> dense(softmax)\n",
    "  * max_seq_len은 문장 리스트중 가장 긴 문장의 길이로 보통\n",
    "* 6.3 개체명 인식을 위한 양방향 LSTM 모델\n",
    "  * 순차데이터를 입력으로 받아 원하는 개체를 출력하도록\n",
    "  * 개체는 단어일 수도 있고, 문장일 수도 있고 설정에 따라 다름"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230629\n",
    "* 개체명 인식(NER; Named Entity Recognition)\n",
    "  * 챗봇에서 문장을 정확하게 해석하기 위한 전처리과정\n",
    "  * 문장의 단어가 \"인물\", \"장소\", \"날짜\"등과 같이 개체를 인식하는것\n",
    "  * text.txt\n",
    "    * ;변환전\n",
    "    * $ NER처리 된 후\n",
    "  * BIO 표기법(Beggining, Inside, Outside, 나머지 분류(B_OG, 등))\n",
    "    * B: 개체명이 시작되는 단어에 'B-개체명' 태그\n",
    "    * I: 'B-개체명' 태그와 연결되는 단어, 'I-개체명' 태그\n",
    "    * O: 개체명 외에 모든 단어 태그\n",
    "  * 데이터셋\n",
    "    * BIO 태그 데이터셋: 영어는 많음, 한글은 국립국어원 언어정보나눔터에 공개\n",
    "    * 책 데이터셋: HLCT(2016)를 수정한 KoreanNERCorpus\n",
    "      - github.com/machinereading/KoreanNERCorpus\n",
    "  * preprocessing.text.Tokenizer\n",
    "    * 라벨 인코딩\n",
    "    * Embedding 하기 전 전처리\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 챗봇 엔진 만들기(20230629~)\n",
    "* 챗본 엔진 처리 과정\n",
    "  * 수행과정\n",
    "    1. 질문받음\n",
    "    2. 질문 전처리(불용어 제거, 토큰화)\n",
    "    3. 의도분석\n",
    "    4. 개체명 인식\n",
    "    5. 답변 검색(from 학습 DB)\n",
    "    6. 답변 출력\n",
    "* 챗봇 엔진 만들기\n",
    "  * 학습툴\n",
    "    * 챗봇 엔진의 언어모델 학습 raw 자료 DB화\n",
    "```\n",
    "chatbot --- train_tools --- qna               --- create_train_data_table.py \n",
    "                                                        1. DB에 chatbot_train_data 테이블 헤더 생성\n",
    "                                              --- load_train_data.py \n",
    "                                                        1. DB chatbot_train_data 초기화\n",
    "                                                        2. train_data.xlsx 값 DB에 입력\n",
    "                                              --- train_data.xlsx\n",
    "        --- config      --- DatabaseConfig.py\n",
    "```\n",
    "  * 전처리\n",
    "    * 토큰화, 불용어 처리, 해석용이 처리\n",
    "```\n",
    "chatbot --- utils --- Preprocess.py\n",
    "                              1. Class Preprocess ( word2index_dic, userdict)\n",
    "                                * wordindex_dic = (자료) create_dict.py에서 만든 학습자료로 만든 단어사전 word_index, (용도) 학습자료로 만든 사전으로 입력자료의 라벨인코딩을 하기 위해\n",
    "                                * userdict = 형태소 분석기에 신규단어등록 [단어,태깅] 사전, (용도) Komoran 모듈의 userdic으로 활용\n",
    "                              2. POS 태거\n",
    "                              3. 불용어 제거\n",
    "```\n",
    "  * 인코딩(단어 사전 구축 및 시퀀스 생성)\n",
    "    * DB에 저장된 학습 raw자료 단어사전화\n",
    "```\n",
    "chatbot --- train_tools --- dict --- create_dict.py\n",
    "                                       1. 학습 코퍼스 불러옴\n",
    "                                       2. 토큰화를 통한 단어사전 생성(시퀀스 데이터 추출은 안함)\n",
    "                                       3. 단어사전(tokenizer.word_index('단어': 라벨인코딩)) 바이너리로 저장(chatbot_dict.bin)\n",
    "        --- test        --- chatbot_dict_test.py\n",
    "                                       1. 단어사전(tokenizer.word_index) 바이너리 로드(chatbot_dict.bin)\n",
    "                                       2. 예제 문장 정의\n",
    "                                       3. Preprocess 클래스 인스턴스 생성\n",
    "                                       4. 형태소 태깅\n",
    "```\n",
    "  * 의도분류 모델\n",
    "    * 훈련데이터: cupurs.txt\n",
    "      * 말뭉치: 문장\n",
    "      * 레이블: [ 0: 인사, 1: 욕설, 2: 주문, 3: 예약, 4: 기타 ]\n",
    "    * CNN 기반 의도분류 모델 개발\n",
    "      * 입력: 말뭉치 --> 토큰화(형태소 분리, 라벨인덱싱, 패딩)\n",
    "      * 구조: 임베딩 --> 3-CNN --> concat --> logit_dense(out_dense를 위해 출력개수를 미리 맞춰줌?) --> out_dense(softmax)\n",
    "```\n",
    "chatbot --- model --- intent --- train_model.py\n",
    "```\n",
    "    * 예측모듈 생성(Class IntentModel)\n",
    "      * 초기화: 레이블, 모델, 전처리 초기화\n",
    "      * 예측: 형태소 태깅 --> 불용어처리 --> 라벨인덱싱 --> 패딩 --> 예측 --> 최대확률 인덱스추출\n",
    "```\n",
    "chatbot --- model --- intent --- IntentModel.py\n",
    "```\n",
    "    * 예측 테스트\n",
    "```\n",
    "chatbot --- test --- model_intent_test.py\n",
    "```\n",
    "  * 개체명 인식 모델\n",
    "    * 훈련데이터: ner_train.txt\n",
    "      * 말뭉치: 문장 시퀀스\n",
    "      * 레이블: BIO\n",
    "    * BiLSTM 기반 개체명 인식 모델 개발\n",
    "      * 입력: 말뭉치 --> 토큰화(형태소 분리, 라벨인덱싱, 패딩)\n",
    "      * 구조: 임베딩 --> BiLSTM 1-layer --> Dense(softmax)\n",
    "```\n",
    "chatbot --- model --- ner --- train_model.py\n",
    "```\n",
    "    * 예측모듈\n",
    "      * 초기화: 레이블, 모델, 전처리 초기화\n",
    "      * 예측: 형태소 태깅 --> 불용어처리 --> 라벨인덱싱 --> 패딩 --> 예측 --> 최대확률 인덱스추출 --> index to tag\n",
    "```\n",
    "chatbot --- model --- intent --- NerModel.py\n",
    "```\n",
    "    * 예측 테스트\n",
    "```\n",
    "chatbot --- test --- model_ner_test.py\n",
    "```\n",
    "\n",
    "  * 답변 검색\n",
    "    * DB에 답변 조회 데이터 로드\n",
    "    * DB 제어 모듈\n",
    "      * DB 연결정보 초기화\n",
    "      * DB 연결\n",
    "      * DB 연결 종료\n",
    "      * SQL 문 실행\n",
    "      * SQL 문 실행 후 1개 ROW 리턴\n",
    "      * SQL 문 실행 후 전체 ROW 리턴\n",
    "    * 답변 검색 모듈\n",
    "      * 초기화(DB 받아옴)\n",
    "      * 검색 쿼리문 만들기 \n",
    "        * find sql = where (intent, or intent, ner_tags)\n",
    "      * 검색 쿼리문으로 답변 조회\n",
    "        * db.select_one(find sql)\n",
    "      * ner tag를 한글로 치환하여 답변\n",
    "  * 챗봇 엔진\n",
    "    * 답변 조회 DB 연결\n",
    "    * 질문 인수\n",
    "    * 의도, 개체명 인식 모델 예측 수행 --> 의도_레이블, ner_tag값 반환\n",
    "    * 답변조회\n",
    "    * 답변 제출\n",
    "    * 답변 조히 DB 연결 종료"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chatbot_dict_test.py에서 10=1, 새우볶음밥=1로 나온 이유 \n",
    "\n",
    "* 6장 CNN 모델 입력자료 전처리과정\n",
    "  * 문장 데이터 로드\n",
    "    * corpus.txt(0:인사, 1:욕설, 2:주문, 3:예약, 4:기타)\n",
    "  * 단어 분리\n",
    "    * tensorflow.keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    * 토크나이저 피팅\n",
    "      * preprocessing.text.Tokenizer()\n",
    "      * tokenizer.fit_on_texts(corpus)\n",
    "    * 토크나이저에서 라벨인코딩 데이터 추출\n",
    "      * sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    * word_index 추출\n",
    "      * word_index = tokenizer.word_index\n",
    "    * 패딩\n",
    "\n",
    "* 8장 CNN 모델 입력자료 전처리과정\n",
    "  * 문장 데이터 로드\n",
    "  * 단어 분리\n",
    "    * 전처리 모듈 로드 p = Preprocess(이미 처리된 학습사전(word2index_dic), 형태소 분리를 위한 userdic)\n",
    "    * 단어 시퀀스 생성\n",
    "      * 형태소 분리(userdic 사용)\n",
    "      * 불용어 처리\n",
    "      * 학습 사전으로부터 인덱스 생성(word2index_dic 사용)\n",
    "    * 패딩\n",
    "\n",
    "* 6장과 8장의 차이점\n",
    "  * 과정은 동일한데 tensorflow.keras.preprocessing를 썼냐 안썼냐 차이점\n",
    "  * 추가로 6장에서는 불용어 처리를 하지 않았음\n",
    "\n",
    "* 이유 조사\n",
    "  * corpus.txt\n",
    "    * \"새우볶음밥\", \"10시\" 있음\n",
    "  * user_dic.tsv\n",
    "    * \"새우볶음밥\", \"10시\" 있음\n",
    "  * 처음 사전 생성 create_dict.py\n",
    "    * 형태소 분리\n",
    "      * komoran(userdic에 아무것도 안들어감)\n",
    "      * (새우, 볶음밥, 10, 시) 로 분리됨\n",
    "    * 사전 생성\n",
    "      * p = Preprocess() 했을때, \n",
    "      * 단어사전, word_index에 \"새우볶음밥\" 없음, \"10시\" 없음\n",
    "      * (새우, 볶음밥, 10, 시) 로 분리됨\n",
    "  * 테스트\n",
    "    * p = Preprocess(userdic='../utils/user_dic.tsv')\n",
    "      * 여기서는 user_dic을 사용\n",
    "    * 사용자추가 내용으로 형태소를 분리함\n",
    "    \n",
    "--> 이유 찾기 힘드나 코드상 학습사전 만들때도 user_dict.tsv를 사용해야 되는 것으로 보임\n",
    "--> 해보니 제대로 나옴"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 챗봇 엔진서버 개발\n",
    "  * 다수의 챗봇 서비스가 접속해 화자의 질의에 대답\n",
    "  1. 통신프로토콜\n",
    "    * 프로토콜: 서버와 클라이언트간 통신 규약\n",
    "    * 프로토콜 설정: 챗봇 엔진 서버와 통신하기 위함\n",
    "    * 통신형태: json key,value\n",
    "    * 양방향 통신\n",
    "      * 클라이언트 --> 서버: { \"Query\": \"짜장면 주문할께요\", \"BotType\": \"Kakao\"}\n",
    "      * 서버 --> 클라이언트: { \"Query\": \"짜장면 주문할께요\", \"Intent\": \"주문\", \"NER\": \"...\", \"Answer\": \"짜장면 주문 처리, 감사\", ...}\n",
    "  2. 다중 접속을 위한 TCP 소켓\n",
    "\n",
    "> 소켓(Socket)은 TCP/IP 기반 네트워크 통신에서 데이터 송수신의 마지막 접점을 말합니다. 소켓통신은 이러한 소켓을 통해 서버-클라이언트간 데이터를 주고받는 양방향 연결 지향성 통신을 말합니다. 소켓통신은 보통 지속적으로 연결을 유지하면서 실시간으로 데이터를 주고받아야 하는 경우에는 사용됩니다. 소켓은 클라이언트 소켓과 서버 소켓으로 구분되며, 소켓간 통신을 위해서는 네트워크상에서 클라이언트와 서버에 해당되는 컴퓨터를 식별하기 위한 IP주소와 해당 컴퓨터내에서 현재 통신에 사용되는 응용프로그램을 식별하기 위한 포트번호가 사용됩니다. (출처: https://kadosholy.tistory.com/125)\n",
    "\n",
    "  * 멀티 쓰레드를 이용해서 다수의 서비스 해결\n",
    "  * 서버 자원관리를 위해서 동시 생성 쓰레드수를 지정\n",
    "  * /utils --> BotServer.py\n",
    "  * import socket: 저수준 네트워킹 API를 사용하기 위한 래퍼 함수 \n",
    "  * Class BotServer  \n",
    "    * 초기화\n",
    "      * 포트번호, 연결 수 지정\n",
    "    * TCP/IP 소켓 생성\n",
    "      * 지정한 서버 포트로, 지정한 연결수 만큼 클라이언트 연결을 수락\n",
    "      * 설정 소켓 인스턴스 반환\n",
    "    * 클라이언트 소켓 생성\n",
    "      * TCP/IP 소켓 인스턴스로 부터 클라이언트와 통신할 클라이언트 소켓을 반환\n",
    "      * mySock.accept() --> conn(클라이언트 소켓), address(바인드된 주소)\n",
    "    * 생성된 소켓 반환\n",
    "  * 메인프로그램\n",
    "    * DB를 로드하고 --> 100개의 소켓을 열어두고, 클라이언트를 받음 --> 연결개수? 만큼 쓰레딩 발생 --> to_client 함수 각 쓰레딩마다 배치 --> 작업실행\n",
    "    * 클라이언트가 연결되면 \n",
    "    * 전처리 인스턴스 및 모델 로드\n",
    "  * 클라이언트 테스트\n",
    "    * 메인프로그램이 백그라운드로 실행되고 있는 가운데\n",
    "    * 클라이언트 테스트 프로그램 수행\n",
    "      * 엔진서버에 연결\n",
    "      * 클라이언트 소캣 생성 --> 서버소켓에서 accept()\n",
    "      * 질문 인코딩해서 보내기\n",
    "      * 답변 디코딩해서 받기\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 챗봇 API\n",
    "  * 카카오톡이나 네이버톡톡에서 챗봇 엔진을 수행하는 방법\n",
    "  * REST API 방식으로 API서버 구축\n",
    "    * REST API\n",
    "      * 웹 URI를 통해 자원 할당\n",
    "      * HTTP 메서드를 통해 해당 자원 CRUD\n",
    "        * POST <--> Create\n",
    "        * GET <--> Read\n",
    "        * PUT <--> Update\n",
    "        * DELETE <--> Delete\n",
    "    * 구축 언어\n",
    "      * python Flask: 웹서비스 구현 프레임워크, REST_API 서버 개발에 많이 사용\n",
    "        * hello flask: ex9-1.py\n",
    "        * URI 동적 변수: ex9-2.py\n",
    "        * 기본 REST_API 서비스 구현\n",
    "          * app.py\n",
    "          * Talend API Tester(Chrome 확장)\n",
    "          * HTTP 메서드에 따라 URI 호출\n",
    "  * 챗봇 API 서비스 구현\n",
    "    * 클라이언트 --> API 서버 --> 소켓서버(챗본엔진서버): 응답모듈\n",
    "    * 교재 환경에서는 웹앱이 없기 때문에 클라이언트가 내용을 API로 전달할 수 있는 Talend API Tester"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카카오톡 챗봇 만들기\n",
    "* 카카오 챗봇: 카카오 채널에서 동작하는 챗봇\n",
    "* 카카오 오픈빌더: 카카오의 인공지능 기술을 이용해 챗봇 엔진 구현 없이도 성능 좋은 챗봇을 쉽게 만들 수 있음 --> 하지만 예제에서 만든 엔진으로 사용\n",
    "* 카카오톡 채널 가입 (2~3일)\n",
    "* 카카오 아이 오픈빌더 신청 (5~6일)\n",
    "* 카카오톡 챗봇 연동\n",
    "  * 블록: 사용자 의도(intent) 처리 단위, 사용자 발화가 입력되면 그에 맞는 1개 블록이 실행\n",
    "  * 시나리오: 여러 블록이 하나의 시나리오 구성\n",
    "  * 블록을 직접 엔진으로 구현했기 때문에 카카오톡 시나리오의 블록을 만들어 사용하지 않음\n",
    "  * 시나리오\n",
    "    * 기본 3가지 블록이 제공\n",
    "    * 웰컴: 톡방 입장 웰컴 메세지\n",
    "    * 폴백: 사용자 의도 파악 안되었을 경우\n",
    "    * 탈출: 챗봇이 사용자에게 계속해서 되묻는 상황에서 탈출\n",
    "  * 스킬\n",
    "    * 응답 설정을 사용자 프로그램으로 설정(블록과 유사, 외부기능 삽입)\n",
    "  * 카카오톡 챗봇에 사용자 엔진 탑재 방법\n",
    "    * 기본적으로 외부 챗봇엔진을 지원하지 않음\n",
    "    * 폴백 블록에는 사용자 엔진을 연동시킬 수 있음\n",
    "    * 따라서 기본블록 이외 다른 블록을 생성하지 않아, 폴백에서 사용자 엔진을 사용하도록 만듦\n",
    "    * 폴백블록에서 챗봇 API 서버와 통신할 수 있는 '스킬서버'를 지정하여 사용\n",
    "    * 스킬서버 = rest API 만드는 방식과 동일\n",
    "  * 스킬서버 만들기\n",
    "    * "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
