{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# if gpus:\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         for gpu in gpus:\n",
    "\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "#     except RuntimeError as e:\n",
    "\n",
    "#         print(e)\n",
    "\n",
    "# # 실행가능한 gpu 목록\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "# #실행가능한 cpu, gpu 목록\n",
    "# from tensorflow.python.client import device_lib\n",
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "from tensorflow.keras import Input, Model, callbacks\n",
    "from tensorflow.keras.utils import plot_model as plm\n",
    "from tensorflow.keras.activations import swish\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tcn import TCN, tcn_full_summary\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set configure\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Data set\n",
    "\n",
    "element = 'ALLV'\n",
    "name_list = \"./SHEL/namelist.input\"\n",
    "\n",
    "hp_lr = 0.009\n",
    "hp_pd = 'same'\n",
    "hp_ns = 1\n",
    "hp_dl = [1,2,4,8,16,32,48]\n",
    "hp_ldl = hp_dl[-1] # last dilation factor to make name of save model\n",
    "hp_bn = True\n",
    "hp_nf = 80\n",
    "hp_dr = 0.07\n",
    "hp_ks = 6\n",
    "\n",
    "input_size = 6\n",
    "output_size = 2\n",
    "num_fct = 48\n",
    "batch_size = 8\n",
    "n_iter_search = 20\n",
    "num_epoch = 600\n",
    "dev_stn_id = 47105\n",
    "#dev_stn_id = 875\n",
    "tran_data_per = \"2101_2104_2201_2204\"\n",
    "\n",
    "exp_name = \"CNTL\"\n",
    "csv_outdir = './DAOU/LOSS/' + exp_name + '/'\n",
    "model_outdir = './DAOU/MODL/' + exp_name + '/'\n",
    "scalr_outdir = './DAOU/SCAL/' + exp_name + '/'\n",
    "gifd_outdir = './GIFD/' + exp_name + '/'\n",
    "log_outdir = './DAOU/LOGF/' + exp_name + '/'\n",
    "\n",
    "\n",
    "if os.path.exists(csv_outdir) != True: os.makedirs(csv_outdir)\n",
    "if os.path.exists(model_outdir) != True: os.makedirs(model_outdir)\n",
    "if os.path.exists(scalr_outdir) != True: os.makedirs(scalr_outdir)\n",
    "if os.path.exists(gifd_outdir) != True: os.makedirs(gifd_outdir)\n",
    "if os.path.exists(log_outdir) != True: os.makedirs(log_outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== load data shape\n",
      "(868, 49, 20)\n",
      "(868, 49, 2)\n",
      "================================================== split data shape\n",
      "(109, 49, 20)\n",
      "(109, 49, 2)\n",
      "(61, 49, 20)\n",
      "(61, 49, 2)\n",
      "결측 합계:  1\n",
      "shape of after drop\n",
      "(108, 49, 20)\n",
      "(108, 49, 2)\n",
      "결측 합계:  5\n",
      "shape of after drop\n",
      "(56, 49, 20)\n",
      "(56, 49, 2)\n",
      "================================================== drop data shape\n",
      "(108, 48, 6)\n",
      "(108, 48, 2)\n",
      "(56, 48, 6)\n",
      "(56, 48, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../config/')\n",
    "from global_params import variable_info\n",
    "#from config.global_params import variable_info\n",
    "\n",
    "sys.path.insert(0, './INC/')\n",
    "from data_split import data_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 인풋 준비\n",
    "nwp_file = f\"../DAIO/nwp_data_{dev_stn_id}\"\n",
    "obs_file = f\"../DAIO/obs_data_{dev_stn_id}\"\n",
    "nwp_data = np.load(nwp_file)\n",
    "obs_data = np.load(obs_file)\n",
    "print(\"=\"*50, \"load data shape\")\n",
    "print(nwp_data.shape)\n",
    "print(obs_data.shape)\n",
    "\n",
    "\n",
    "# train([21.01,04, 22.01,04]) / test([23.01,04]) 분할  \n",
    "class_split = data_split(nwp_data, obs_data)\n",
    "train_nwp, test_nwp, train_obs, test_obs = class_split.get_split_data()\n",
    "print(\"=\"*50, \"split data shape\")\n",
    "print(train_nwp.shape)\n",
    "print(train_obs.shape)\n",
    "print(test_nwp.shape)\n",
    "print(test_obs.shape)\n",
    "\n",
    "\n",
    "\n",
    "# 결측제거\n",
    "missing_nwp_train = set(np.where(np.isnan(train_nwp))[0])\n",
    "missing_obs_train = set(np.where(np.isnan(train_obs))[0])\n",
    "missing_all_train = list(missing_nwp_train | missing_obs_train)\n",
    "print(\"결측 합계: \", len(missing_all_train))\n",
    "dm_nwp_train = np.delete(train_nwp, missing_all_train, 0)\n",
    "dm_obs_train = np.delete(train_obs, missing_all_train, 0)\n",
    "print(\"shape of after drop\")\n",
    "print(dm_nwp_train.shape)\n",
    "print(dm_obs_train.shape)\n",
    "\n",
    "missing_nwp_test = set(np.where(np.isnan(test_nwp))[0])\n",
    "missing_obs_test = set(np.where(np.isnan(test_obs))[0])\n",
    "missing_all_test = list(missing_nwp_test | missing_obs_test)\n",
    "print(\"결측 합계: \", len(missing_all_test))\n",
    "dm_nwp_test = np.delete(test_nwp, missing_all_test, 0)\n",
    "dm_obs_test = np.delete(test_obs, missing_all_test, 0)\n",
    "print(\"shape of after drop\")\n",
    "print(dm_nwp_test.shape)\n",
    "print(dm_obs_test.shape)\n",
    "\n",
    "\n",
    "# 변수선택\n",
    "# for 47105\n",
    "#sel_var = ['NDNSW_surface', 'UGRD_10m', 'VGRD_10m', 'RH_1_5ma', 'MAXGUST_0m', 'PRMSL_meansealevel']\n",
    "# for 875\n",
    "sel_var = ['UGRD_10m', 'VGRD_10m', \"TMP_1_5m\", 'RH_1_5ma', 'PRMSL_meansealevel', \"PRES_surface\"]\n",
    "\n",
    "var_list_dict = list(variable_info.keys())\n",
    "var_index = [ var_list_dict.index(i) for i in sel_var ]\n",
    "#print(var_list_dict)\n",
    "#print(var_index)\n",
    "sel_dm_nwp_train = dm_nwp_train[:,1::,var_index]\n",
    "sel_dm_nwp_test = dm_nwp_test[:,1::,var_index]\n",
    "dm_obs_train = dm_obs_train[:,1::,:]\n",
    "dm_obs_test = dm_obs_test[:,1::,:]\n",
    "\n",
    "\n",
    "print(\"=\"*50, \"drop data shape\")\n",
    "print(sel_dm_nwp_train.shape)\n",
    "print(dm_obs_train.shape)\n",
    "print(sel_dm_nwp_test.shape)\n",
    "print(dm_obs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Final training data shape\n",
      "<class 'numpy.ndarray'>\n",
      "tran nwp :  (108, 48, 6)\n",
      "tran obs :  (108, 48, 2)\n",
      "test nwp :  (56, 48, 6)\n",
      "test obs :  (56, 48, 2)\n",
      "in split, re_nwp_stn shape:  (108, 48, 6)\n",
      "tran nwp shape:  (86, 48, 6) tran obs shape:  (86, 48, 2)\n",
      "vald nwp shape:  (22, 48, 6) vald obs shape:  (22, 48, 2)\n",
      "dropna_nwp shape: (86, 48, 6)\n",
      "dropna_obs shape: (86, 48, 2)\n",
      "calc from numpy\n",
      "tnwp_x_median: 0.4106244214954994\n",
      "tobs_x_median: 0.4724702380952381\n",
      "./GIFD/CNTL/KDE_ecmw_tran_var6_2101_2104_2201_2204_47105\n",
      "dropna_nwp shape: (22, 48, 6)\n",
      "dropna_obs shape: (22, 48, 2)\n",
      "calc from numpy\n",
      "tnwp_x_median: 0.3939926076780676\n",
      "tobs_x_median: 0.4540816326530611\n",
      "./GIFD/CNTL/KDE_ecmw_vald_var6_2101_2104_2201_2204_47105\n",
      "dropna_nwp shape: (86, 48, 6)\n",
      "dropna_obs shape: (22, 48, 6)\n",
      "calc from numpy\n",
      "tnwp_x_median: 0.4106244214954994\n",
      "tobs_x_median: 0.3939926076780676\n",
      "./GIFD/CNTL/KDE_ecmw_tran_vald_nwp_2101_2104_2201_2204_47105\n",
      "dropna_nwp shape: (86, 48, 2)\n",
      "dropna_obs shape: (22, 48, 2)\n",
      "calc from numpy\n",
      "tnwp_x_median: 0.5014880952380952\n",
      "tobs_x_median: 0.4680059523809524\n",
      "./GIFD/CNTL/KDE_ecmw_tran_vald_obs_2101_2104_2201_2204_47105\n"
     ]
    }
   ],
   "source": [
    "# .. 스케일링 및 데이터 분할\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './INC')\n",
    "from step_sampling_for_date import step_sampling_for_date\n",
    "from hist_and_kde_for_split import hist_and_kde_for_split, hist_and_kde_for_split_UV\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Normalize\n",
    "\n",
    "output_size = 2\n",
    "\n",
    "# .. initialaize\n",
    "tr_b, tr_s, tr_f = sel_dm_nwp_train.shape[0], sel_dm_nwp_train.shape[1], sel_dm_nwp_train.shape[2]      \n",
    "ts_b, ts_s, ts_f = sel_dm_nwp_test.shape[0], sel_dm_nwp_test.shape[1], sel_dm_nwp_test.shape[2]      \n",
    "\n",
    "# .. get restorator with obs range\n",
    "nwp_scaler = MinMaxScaler()   # copy default true\n",
    "obs_scaler = MinMaxScaler()\n",
    "nwp_scaler.fit(sel_dm_nwp_train.view().reshape(tr_b*tr_s, tr_f))\n",
    "obs_scaler.fit(dm_obs_train.view().reshape(tr_b*tr_s, output_size))\n",
    "\n",
    "# .. feature normalize   ( train seq, feature = test seq, feature )\n",
    "nor_dm_nwp_train = nwp_scaler.transform(sel_dm_nwp_train.reshape(tr_b*tr_s, tr_f))\n",
    "nor_dm_nwp_train = nor_dm_nwp_train.reshape(tr_b,tr_s,tr_f)\n",
    "nor_dm_nwp_test = nwp_scaler.transform(sel_dm_nwp_test.reshape(ts_b*ts_s, ts_f))\n",
    "nor_dm_nwp_test = nor_dm_nwp_test.reshape(ts_b,ts_s,ts_f)\n",
    "\n",
    "nor_dm_obs_train = obs_scaler.transform(dm_obs_train.reshape(tr_b*tr_s, output_size))\n",
    "nor_dm_obs_train = nor_dm_obs_train.reshape(tr_b,tr_s, output_size)\n",
    "nor_dm_obs_test = obs_scaler.transform(dm_obs_test.reshape(ts_b*ts_s, output_size))\n",
    "nor_dm_obs_test = nor_dm_obs_test.reshape(ts_b,ts_s, output_size)\n",
    "\n",
    "\n",
    "print ('---------- Final training data shape')\n",
    "print(type(nor_dm_nwp_train))\n",
    "print ('tran nwp : ', nor_dm_nwp_train.shape)\n",
    "print ('tran obs : ', nor_dm_obs_train.shape)\n",
    "print ('test nwp : ', nor_dm_nwp_test.shape)\n",
    "print ('test obs : ', nor_dm_obs_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. split train --> train/valid for hyper-parameter tuning\n",
    "#tran_x, tran_y, vald_x, vald_y = data_split_5years(re_nwp_stn, re_obs_stn, tran_rate, random_seed) \n",
    "tran_rate = 0.8\n",
    "nbin = 10\n",
    "random_seed = 1\n",
    "tran_x, tran_y, vald_x, vald_y = step_sampling_for_date(nor_dm_nwp_train, nor_dm_obs_train, tran_rate, nbin, random_seed)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Plot tran valid\n",
    "\n",
    "#var_name = ['NDNSW_surface', 'UGRD_10m', 'VGRD_10m', 'RH_1_5ma', 'MAXGUST_0m', 'PRMSL_meansealevel', \"OBS\"]\n",
    "exp_name = \"stn875\"\n",
    "\n",
    "#hist_and_kde_for_split_UV(exp_name, tran_data_per, dev_stn_id, gifd_outdir, tran_y, vald_y, nor_dm_obs_test)                      \n",
    "hist_and_kde_for_split(exp_name, exp_name, tran_data_per, dev_stn_id, gifd_outdir, tran_x, tran_y, vald_x, vald_y)\n",
    "#hist_and_kde_for_split(var_name, exp_name, tran_data_per, dev_stn_id, gifd_outdir,\n",
    "#                       tran_x, tran_y, vald_x, vald_y )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size:  6\n",
      "batch_size:  8\n",
      "time_lenght:  48\n"
     ]
    }
   ],
   "source": [
    "#=========================================================================\n",
    "# .. Model configuration\n",
    "import tensorflow_addons as tfa\n",
    "                \n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set batch for cross-validation\n",
    "\n",
    "print ('input_size: ', input_size)\n",
    "print ('batch_size: ', batch_size)\n",
    "print ('time_lenght: ', num_fct)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set Model\n",
    "\n",
    "\n",
    "# .. Define model\n",
    "#def create_model(dropout_rate=0.15, nb_filters=7, kernel_size=3): \n",
    "def model_builder(hp): \n",
    "\n",
    "        # set called hyper_params set\n",
    "        hp_nbfilters = hp.Int('nb_filters', min_value = 50, max_value = 100, step = 5)\n",
    "        hp_kernel_size = hp.Int('kernel_size', min_value = 3, max_value = 12, step = 3)\n",
    "        hp_dilation = hp.Int('dilation', min_value = 0, max_value = 4, step = 1)\n",
    "        #hp_dilation = hp.Choice('dilation', values = [[1,2,4], [1,2,4,8], [1,2,4,8,16], [1,2,4,8,16,32], [1,2,4,8,16,32,48]])\n",
    "\n",
    "        dilation_list = [[1,2,4], [1,2,4,8], [1,2,4,8,16], [1,2,4,8,16,32], [1,2,4,8,16,32,48]]\n",
    "        dilation = dilation_list[hp_dilation][:]\n",
    "\n",
    "        \n",
    "\n",
    "        print ('================== Model called ========================')\n",
    "        print ('input_size: ', input_size)\n",
    "        print ('batch_size: ', batch_size)\n",
    "        print ('time_lenght: ', num_fct)\n",
    "        print ('nb_filters: ', hp_nbfilters)\n",
    "        print ('kernel_size: ', hp_kernel_size)\n",
    "        print ('dilations: ', dilation)\n",
    "        dropout_rate = np.round(hp_dr,2)\n",
    "        print ('dropout_rate: ', dropout_rate)\n",
    "        \n",
    "        ## .. clear keras model\n",
    "        K.clear_session()\n",
    "        # .. create model\n",
    "        #i = Input( batch_shape=(batch_size, num_fct, input_size) )\n",
    "        i = Input( batch_shape=(None, num_fct, input_size) )\n",
    "        o = TCN(return_sequences=True,\n",
    "                activation=swish,\n",
    "                nb_filters=hp_nbfilters,\n",
    "                padding=hp_pd,\n",
    "                use_batch_norm = hp_bn,\n",
    "                nb_stacks=hp_ns,\n",
    "                dropout_rate=hp_dr,\n",
    "                kernel_size=hp_kernel_size,\n",
    "                use_skip_connections=True,\n",
    "                dilations=dilation\n",
    "                )(i)\n",
    "        o = TimeDistributed(Dense(output_size, activation='linear'))(o)\n",
    "        # .. compile\n",
    "        adam = optimizers.Adam(learning_rate=hp_lr)\n",
    "        m= Model(inputs=[i], outputs=[o])\n",
    "        m.compile(optimizer=adam, loss='mse', metrics=[tfa.metrics.RSquare(name='r_square')])\n",
    "        #m.compile(optimizer=adam, loss='mse', metrics=[r2_3dim_uv])\n",
    "        m.summary()\n",
    "        return m\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Model called ========================\n",
      "input_size:  6\n",
      "batch_size:  8\n",
      "time_lenght:  48\n",
      "nb_filters:  50\n",
      "kernel_size:  3\n",
      "dilations:  [1, 2, 4]\n",
      "dropout_rate:  0.07\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 48, 6)]           0         \n",
      "                                                                 \n",
      " tcn (TCN)                   (None, 48, 50)            40250     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 48, 2)            102       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,352\n",
      "Trainable params: 39,752\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "Search space summary\n",
      "Default search space size: 3\n",
      "nb_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 50, 'max_value': 100, 'step': 5, 'sampling': 'linear'}\n",
      "kernel_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 3, 'max_value': 12, 'step': 3, 'sampling': 'linear'}\n",
      "dilation (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 0, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# pip install keras_tuner\n",
    "# pip install keras_tuner_cv\n",
    "#from keras_tuner_cv.outer_cv import OuterCV\n",
    "#from keras_tuner.tuners import RandomSearch\n",
    "#from sklearn.model_selection import KFold\n",
    "import IPython\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "#from keras_tuner_cv.outer_cv import OuterCV\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set metrics\n",
    "#tf.keras.metrics.R\n",
    "#f1 = tfa.metrics.F1Score(num_classes = len(CLASSES), average='macro')\n",
    "\n",
    "#budget = 9\n",
    "#exp_name = 'B9'\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Use bayes_opt\n",
    "\n",
    "# tuner = kt.Hyperband(hypermodel = model_builder,\n",
    "#                     objective = kt.Objective('val_r_square', direction='max'), \n",
    "#                     max_epochs = budget,\n",
    "#                     factor = 3,\n",
    "#                     #overwrite=True,\n",
    "#                     directory = './OPTM/',\n",
    "#                     project_name = exp_name)\n",
    "\n",
    "tuner = kt.BayesianOptimization(model_builder,\n",
    "                     objective = kt.Objective('val_r_square', direction='max'),\n",
    "                     overwrite=True,\n",
    "                     max_trials=50)\n",
    "\n",
    "\n",
    "# 작성한 Hypermodel 출력\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 01m 44s]\n",
      "val_r_square: 0.33813273906707764\n",
      "\n",
      "Best val_r_square So Far: 0.3725329339504242\n",
      "Total elapsed time: 01h 34m 34s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Hyperband search took 5673.93 seconds parameter settings. \n"
     ]
    }
   ],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)\n",
    "\n",
    "\n",
    "start = time()\n",
    "tuner.search(tran_x, tran_y, epochs = 1000, validation_data = (vald_x, vald_y), callbacks = [ClearTrainingOutput()])\n",
    "#tuner.search(tran_x, tran_y, epochs = 600, validation_data = (vald_x, vald_y))\n",
    "\n",
    "print(\"Hyperband search took %.2f seconds\"     \" parameter settings. \" % ((time() - start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "best_hps\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "    best_kernel_size= {best_hps.get('kernel_size')} \n",
    "    best_nb_filters= {best_hps.get('nb_filters')}\n",
    "    best_dilations= {best_hps.get('dilation')}\n",
    "\"\"\")\n",
    "\n",
    "# 혹은 결과 출력\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_r_square\", direction=\"max\")\n",
      "\n",
      "Trial 40 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 100\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.3725329339504242\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 100\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.36801132559776306\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 90\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.36554673314094543\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 60\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.36310628056526184\n",
      "\n",
      "Trial 35 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 100\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.361371785402298\n",
      "\n",
      "Trial 47 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 100\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.36121582984924316\n",
      "\n",
      "Trial 25 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 55\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.35837626457214355\n",
      "\n",
      "Trial 48 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 50\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.35758647322654724\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 50\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.3544951379299164\n",
      "\n",
      "Trial 33 summary\n",
      "Hyperparameters:\n",
      "nb_filters: 100\n",
      "kernel_size: 3\n",
      "dilation: 0\n",
      "Score: 0.35294970870018005\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
