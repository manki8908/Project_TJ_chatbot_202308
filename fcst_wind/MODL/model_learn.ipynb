{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편차보정모델 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로드, 결측제거, 변수 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== load data shape\n",
      "(868, 49, 20)\n",
      "(868, 49, 2)\n",
      "================================================== split data shape\n",
      "(109, 49, 20)\n",
      "(109, 49, 2)\n",
      "(61, 49, 20)\n",
      "(61, 49, 2)\n",
      "결측 합계:  1\n",
      "shape of after drop\n",
      "(108, 49, 20)\n",
      "(108, 49, 2)\n",
      "결측 합계:  5\n",
      "shape of after drop\n",
      "(56, 49, 20)\n",
      "(56, 49, 2)\n",
      "================================================== drop data shape\n",
      "(108, 49, 6)\n",
      "(108, 49, 2)\n",
      "(56, 49, 6)\n",
      "(56, 49, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_split import data_split\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from config.global_params import variable_info\n",
    "\n",
    "\n",
    "# 인풋 준비\n",
    "nwp_file = \"../DAIO/nwp_data_47105\"\n",
    "obs_file = \"../DAIO/obs_data_47105\"\n",
    "nwp_data = np.load(nwp_file)\n",
    "obs_data = np.load(obs_file)\n",
    "print(\"=\"*50, \"load data shape\")\n",
    "print(nwp_data.shape)\n",
    "print(obs_data.shape)\n",
    "\n",
    "\n",
    "# train([21.01,04, 22.01,04]) / test([23.01,04]) 분할  \n",
    "class_split = data_split(nwp_data, obs_data)\n",
    "train_nwp, test_nwp, train_obs, test_obs = class_split.get_split_data()\n",
    "print(\"=\"*50, \"split data shape\")\n",
    "print(train_nwp.shape)\n",
    "print(train_obs.shape)\n",
    "print(test_nwp.shape)\n",
    "print(test_obs.shape)\n",
    "\n",
    "\n",
    "\n",
    "# 결측제거\n",
    "missing_nwp_train = set(np.where(np.isnan(train_nwp))[0])\n",
    "missing_obs_train = set(np.where(np.isnan(train_obs))[0])\n",
    "missing_all_train = list(missing_nwp_train | missing_obs_train)\n",
    "print(\"결측 합계: \", len(missing_all_train))\n",
    "dm_nwp_train = np.delete(train_nwp, missing_all_train, 0)\n",
    "dm_obs_train = np.delete(train_obs, missing_all_train, 0)\n",
    "print(\"shape of after drop\")\n",
    "print(dm_nwp_train.shape)\n",
    "print(dm_obs_train.shape)\n",
    "\n",
    "missing_nwp_test = set(np.where(np.isnan(test_nwp))[0])\n",
    "missing_obs_test = set(np.where(np.isnan(test_obs))[0])\n",
    "missing_all_test = list(missing_nwp_test | missing_obs_test)\n",
    "print(\"결측 합계: \", len(missing_all_test))\n",
    "dm_nwp_test = np.delete(test_nwp, missing_all_test, 0)\n",
    "dm_obs_test = np.delete(test_obs, missing_all_test, 0)\n",
    "print(\"shape of after drop\")\n",
    "print(dm_nwp_test.shape)\n",
    "print(dm_obs_test.shape)\n",
    "\n",
    "\n",
    "# 변수선택\n",
    "sel_var = ['NDNSW_surface', 'UGRD_10m', 'VGRD_10m', 'RH_1_5ma', 'MAXGUST_0m', 'PRMSL_meansealevel']\n",
    "var_list_dict = list(variable_info.keys())\n",
    "var_index = [ var_list_dict.index(i) for i in sel_var ]\n",
    "#print(var_list_dict)\n",
    "#print(var_index)\n",
    "sel_dm_nwp_train = dm_nwp_train[:,:,var_index]\n",
    "sel_dm_nwp_test = dm_nwp_test[:,:,var_index]\n",
    "print(\"=\"*50, \"drop data shape\")\n",
    "print(sel_dm_nwp_train.shape)\n",
    "print(dm_obs_train.shape)\n",
    "print(sel_dm_nwp_test.shape)\n",
    "print(dm_obs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Final training data shape\n",
      "<class 'numpy.ndarray'>\n",
      "tran nwp :  (108, 48, 6)\n",
      "tran obs :  (108, 48, 2)\n",
      "test nwp :  (56, 48, 6)\n",
      "test obs :  (56, 48, 2)\n"
     ]
    }
   ],
   "source": [
    "# .. 스케일링 및 데이터 분할\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Normalize\n",
    "\n",
    "output_size = 2\n",
    "\n",
    "# .. initialaize\n",
    "tr_b, tr_s, tr_f = sel_dm_nwp_train.shape[0], sel_dm_nwp_train.shape[1], sel_dm_nwp_train.shape[2]      \n",
    "ts_b, ts_s, ts_f = sel_dm_nwp_test.shape[0], sel_dm_nwp_test.shape[1], sel_dm_nwp_test.shape[2]      \n",
    "\n",
    "# .. get restorator with obs range\n",
    "nwp_scaler = MinMaxScaler()   # copy default true\n",
    "obs_scaler = MinMaxScaler()\n",
    "nwp_scaler.fit(sel_dm_nwp_train.view().reshape(tr_b*tr_s, tr_f))\n",
    "obs_scaler.fit(dm_obs_train.view().reshape(tr_b*tr_s, output_size))\n",
    "\n",
    "# .. feature normalize   ( train seq, feature = test seq, feature )\n",
    "nor_dm_nwp_train = nwp_scaler.transform(sel_dm_nwp_train.reshape(tr_b*tr_s, tr_f))\n",
    "nor_dm_nwp_train = nor_dm_nwp_train.reshape(tr_b,tr_s,tr_f)\n",
    "nor_dm_obs_train = obs_scaler.transform(dm_obs_train.reshape(tr_b*tr_s, output_size))\n",
    "nor_dm_obs_train = nor_dm_obs_train.reshape(tr_b,tr_s, output_size)\n",
    "\n",
    "nor_dm_nwp_test = nwp_scaler.transform(sel_dm_nwp_test.reshape(ts_b*ts_s, ts_f))\n",
    "nor_dm_nwp_test = nor_dm_nwp_test.reshape(ts_b,ts_s,ts_f)\n",
    "nor_dm_obs_test = obs_scaler.transform(dm_obs_test.reshape(ts_b*ts_s, output_size))\n",
    "nor_dm_obs_test = nor_dm_obs_test.reshape(ts_b,ts_s, output_size)\n",
    "\n",
    "nor_dm_nwp_train = nor_dm_nwp_train[:,1::,:]\n",
    "nor_dm_obs_train = nor_dm_obs_train[:,1::,:]\n",
    "\n",
    "nor_dm_nwp_test = nor_dm_nwp_test[:,1::,:]\n",
    "nor_dm_obs_test = nor_dm_obs_test[:,1::,:]\n",
    "\n",
    "print ('---------- Final training data shape')\n",
    "print(type(nor_dm_nwp_train))\n",
    "print ('tran nwp : ', nor_dm_nwp_train.shape)\n",
    "print ('tran obs : ', nor_dm_obs_train.shape)\n",
    "print ('test nwp : ', nor_dm_nwp_test.shape)\n",
    "print ('test obs : ', nor_dm_obs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 48, 6)]           0         \n",
      "                                                                 \n",
      " tcn_2 (TCN)                 (None, 48, 80)            508240    \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 48, 2)             162       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 508402 (1.94 MB)\n",
      "Trainable params: 506162 (1.93 MB)\n",
      "Non-trainable params: 2240 (8.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./DAOU/SCAL/CNTL/obs_sclr_var6_e1000_bs8_lr0.009_nf80_pdsame_ks6_dr0.07_dl48_ns1_2101_2104_2201_2204_47105.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import plot_model as plm\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tensorflow.keras.activations import swish\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "#import keras\n",
    "#from keras.wrappers.scikit_learn import KerasRegressor\n",
    "#from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "from tensorflow.keras import Input, Model, callbacks\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set configure\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Data set\n",
    "\n",
    "element = 'ALLV'\n",
    "name_list = \"./SHEL/namelist.input\"\n",
    "\n",
    "hp_lr = 0.009\n",
    "hp_pd = 'same'\n",
    "hp_ns = 1\n",
    "hp_dl = [1,2,4,8,16,32,48]\n",
    "hp_ldl = hp_dl[-1] # last dilation factor to make name of save model\n",
    "hp_bn = True\n",
    "hp_nf = 80\n",
    "hp_dr = 0.07\n",
    "hp_ks = 6\n",
    "\n",
    "input_size = 6\n",
    "output_size = 2\n",
    "num_fct = 48\n",
    "batch_size = 8\n",
    "n_iter_search = 20\n",
    "num_epoch = 1000\n",
    "dev_stn_id = 47105\n",
    "tran_data_per = \"2101_2104_2201_2204\"\n",
    "\n",
    "exp_name = \"CNTL\"\n",
    "csv_outdir = './DAOU/LOSS/' + exp_name + '/'\n",
    "model_outdir = './DAOU/MODL/' + exp_name + '/'\n",
    "scalr_outdir = './DAOU/SCAL/' + exp_name + '/'\n",
    "gifd_outdir = './GIFD/' + exp_name + '/'\n",
    "log_outdir = './DAOU/LOGF/' + exp_name + '/'\n",
    "\n",
    "\n",
    "if os.path.exists(csv_outdir) != True: os.makedirs(csv_outdir)\n",
    "if os.path.exists(model_outdir) != True: os.makedirs(model_outdir)\n",
    "if os.path.exists(scalr_outdir) != True: os.makedirs(scalr_outdir)\n",
    "if os.path.exists(gifd_outdir) != True: os.makedirs(gifd_outdir)\n",
    "if os.path.exists(log_outdir) != True: os.makedirs(log_outdir)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set Model\n",
    "\n",
    "# .. Set batch for whole data\n",
    "batch_size = 8\n",
    "\n",
    "# .. create model - API\n",
    "i = Input( batch_shape=(None, num_fct, input_size) ) \n",
    "o = TCN(return_sequences=True, \n",
    "        activation=swish, \n",
    "        nb_filters=hp_nf, \n",
    "        padding=hp_pd,\n",
    "        use_batch_norm = hp_bn,\n",
    "        nb_stacks=hp_ns,\n",
    "        dropout_rate=hp_dr,\n",
    "        kernel_size=hp_ks,\n",
    "        use_skip_connections=True,\n",
    "        dilations=hp_dl\n",
    "        )(i)\n",
    "o = TimeDistributed(Dense(output_size, activation='linear'))(o)\n",
    "\n",
    "adam = optimizers.Adam(learning_rate=hp_lr)\n",
    "m= Model(inputs=[i], outputs=[o])\n",
    "m.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "#tcn_full_summary(m, expand_residual_blocks=True)\n",
    "m.summary()\n",
    "plm(m, to_file='./GIFD/tcn_exp.png', show_shapes=True)\n",
    "\n",
    "\n",
    "#=========================================================================\n",
    "# .. Save model\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set model label\n",
    "\n",
    "make_option = '_e' + str(num_epoch) + '_bs' + str(batch_size) + '_lr' + str(hp_lr) + \\\n",
    "              '_nf' + str(hp_nf) + '_pd' + hp_pd + '_ks' + str(hp_ks) + '_dr'+str(hp_dr) + \\\n",
    "              '_dl' + str(hp_ldl) + '_ns' + str(hp_ns) \n",
    "model_name = 'tcn_modl_' + 'var' + str(input_size) + make_option + \\\n",
    "             '_' + tran_data_per + '_' + str(dev_stn_id) + '.h5'\n",
    "loss_name = 'loss_' + 'var' + str(input_size) + make_option + \\\n",
    "             '_' + tran_data_per + '_' + str(dev_stn_id)\n",
    "scalr_name = 'sclr_' + 'var' + str(input_size) + make_option + \\\n",
    "             '_' + tran_data_per + '_' + str(dev_stn_id) + '.pkl'\n",
    "log_name = 'log_' + 'var' + str(input_size) + make_option + \\\n",
    "             '_' + tran_data_per + '_' + str(dev_stn_id) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. save scaler\n",
    "joblib.dump(nwp_scaler, scalr_outdir + 'nwp_' + scalr_name)\n",
    "joblib.dump(obs_scaler, scalr_outdir + 'obs_' + scalr_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 5.1768WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 7s 26ms/step - loss: 5.0311\n",
      "Epoch 2/1000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9502WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9409\n",
      "Epoch 3/1000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5178WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5161\n",
      "Epoch 4/1000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3025WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2992\n",
      "Epoch 5/1000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2519WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2482\n",
      "Epoch 6/1000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2077WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2098\n",
      "Epoch 7/1000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1905WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 26ms/step - loss: 0.1882\n",
      "Epoch 8/1000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1562WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 26ms/step - loss: 0.1534\n",
      "Epoch 9/1000\n",
      " 3/14 [=====>........................] - ETA: 0s - loss: 0.2623"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m callbacks_list \u001b[39m=\u001b[39m [ callbacks\u001b[39m.\u001b[39mModelCheckpoint(filepath\u001b[39m=\u001b[39mmodel_outdir \u001b[39m+\u001b[39m model_name, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) ]\n\u001b[0;32m      6\u001b[0m csv_logger \u001b[39m=\u001b[39m CSVLogger(log_name, append\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, separator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m hist \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39;49mfit(nor_dm_nwp_train, nor_dm_obs_train,\n\u001b[0;32m      9\u001b[0m       epochs\u001b[39m=\u001b[39;49mnum_epoch,\n\u001b[0;32m     10\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[0;32m     11\u001b[0m       callbacks\u001b[39m=\u001b[39;49mcallbacks_list,\n\u001b[0;32m     12\u001b[0m       shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     13\u001b[0m       \u001b[39m#validation_data=(nor_vald_x, nor_vald_y) )\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m# loss_df = pd.DataFrame(list(zip(hist.history['loss'],hist.history['val_loss'])),\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m#                        columns=[\"tran\",\"eval\"])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\u001b[39mlist\u001b[39m(hist\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[0;32m     22\u001b[0m                        columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtran\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "# .. training model\n",
    "\n",
    "\n",
    "callbacks_list = [ callbacks.ModelCheckpoint(filepath=model_outdir + model_name, monitor='val_loss', save_best_only=True, verbose=1) ]\n",
    "csv_logger = CSVLogger(log_outdir + log_name, append=True, separator=';')\n",
    "\n",
    "hist = m.fit(nor_dm_nwp_train, nor_dm_obs_train,\n",
    "      epochs=num_epoch,\n",
    "      batch_size=batch_size, \n",
    "      callbacks=callbacks_list,\n",
    "      shuffle=True)\n",
    "      #validation_data=(nor_vald_x, nor_vald_y) )\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Save Loss history of best model\n",
    "\n",
    "# loss_df = pd.DataFrame(list(zip(hist.history['loss'],hist.history['val_loss'])),\n",
    "#                        columns=[\"tran\",\"eval\"])\n",
    "loss_df = pd.DataFrame(list(hist.history['loss']),\n",
    "                       columns=[\"tran\"])\n",
    "loss_df.to_csv(csv_outdir+loss_name)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Clear model\n",
    "\n",
    "K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
