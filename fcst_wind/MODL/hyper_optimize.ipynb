{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# if gpus:\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         for gpu in gpus:\n",
    "\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "#     except RuntimeError as e:\n",
    "\n",
    "#         print(e)\n",
    "\n",
    "# # 실행가능한 gpu 목록\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "# #실행가능한 cpu, gpu 목록\n",
    "# from tensorflow.python.client import device_lib\n",
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== load data shape\n",
      "(868, 49, 20)\n",
      "(868, 49, 2)\n",
      "================================================== split data shape\n",
      "(109, 49, 20)\n",
      "(109, 49, 2)\n",
      "(61, 49, 20)\n",
      "(61, 49, 2)\n",
      "결측 합계:  1\n",
      "shape of after drop\n",
      "(108, 49, 20)\n",
      "(108, 49, 2)\n",
      "결측 합계:  5\n",
      "shape of after drop\n",
      "(56, 49, 20)\n",
      "(56, 49, 2)\n",
      "================================================== drop data shape\n",
      "(108, 49, 6)\n",
      "(108, 49, 2)\n",
      "(56, 49, 6)\n",
      "(56, 49, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_split import data_split\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from config.global_params import variable_info\n",
    "\n",
    "\n",
    "# 인풋 준비\n",
    "nwp_file = \"../DAIO/nwp_data_47105\"\n",
    "obs_file = \"../DAIO/obs_data_47105\"\n",
    "nwp_data = np.load(nwp_file)\n",
    "obs_data = np.load(obs_file)\n",
    "print(\"=\"*50, \"load data shape\")\n",
    "print(nwp_data.shape)\n",
    "print(obs_data.shape)\n",
    "\n",
    "\n",
    "# train([21.01,04, 22.01,04]) / test([23.01,04]) 분할  \n",
    "class_split = data_split(nwp_data, obs_data)\n",
    "train_nwp, test_nwp, train_obs, test_obs = class_split.get_split_data()\n",
    "print(\"=\"*50, \"split data shape\")\n",
    "print(train_nwp.shape)\n",
    "print(train_obs.shape)\n",
    "print(test_nwp.shape)\n",
    "print(test_obs.shape)\n",
    "\n",
    "\n",
    "\n",
    "# 결측제거\n",
    "missing_nwp_train = set(np.where(np.isnan(train_nwp))[0])\n",
    "missing_obs_train = set(np.where(np.isnan(train_obs))[0])\n",
    "missing_all_train = list(missing_nwp_train | missing_obs_train)\n",
    "print(\"결측 합계: \", len(missing_all_train))\n",
    "dm_nwp_train = np.delete(train_nwp, missing_all_train, 0)\n",
    "dm_obs_train = np.delete(train_obs, missing_all_train, 0)\n",
    "print(\"shape of after drop\")\n",
    "print(dm_nwp_train.shape)\n",
    "print(dm_obs_train.shape)\n",
    "\n",
    "missing_nwp_test = set(np.where(np.isnan(test_nwp))[0])\n",
    "missing_obs_test = set(np.where(np.isnan(test_obs))[0])\n",
    "missing_all_test = list(missing_nwp_test | missing_obs_test)\n",
    "print(\"결측 합계: \", len(missing_all_test))\n",
    "dm_nwp_test = np.delete(test_nwp, missing_all_test, 0)\n",
    "dm_obs_test = np.delete(test_obs, missing_all_test, 0)\n",
    "print(\"shape of after drop\")\n",
    "print(dm_nwp_test.shape)\n",
    "print(dm_obs_test.shape)\n",
    "\n",
    "\n",
    "# 변수선택\n",
    "sel_var = ['NDNSW_surface', 'UGRD_10m', 'VGRD_10m', 'RH_1_5ma', 'MAXGUST_0m', 'PRMSL_meansealevel']\n",
    "var_list_dict = list(variable_info.keys())\n",
    "var_index = [ var_list_dict.index(i) for i in sel_var ]\n",
    "#print(var_list_dict)\n",
    "#print(var_index)\n",
    "sel_dm_nwp_train = dm_nwp_train[:,:,var_index]\n",
    "sel_dm_nwp_test = dm_nwp_test[:,:,var_index]\n",
    "print(\"=\"*50, \"drop data shape\")\n",
    "print(sel_dm_nwp_train.shape)\n",
    "print(dm_obs_train.shape)\n",
    "print(sel_dm_nwp_test.shape)\n",
    "print(dm_obs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Final training data shape\n",
      "<class 'numpy.ndarray'>\n",
      "tran nwp :  (108, 48, 6)\n",
      "tran obs :  (108, 48, 2)\n",
      "test nwp :  (56, 48, 6)\n",
      "test obs :  (56, 48, 2)\n"
     ]
    }
   ],
   "source": [
    "# .. 스케일링 및 데이터 분할\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Normalize\n",
    "\n",
    "output_size = 2\n",
    "\n",
    "# .. initialaize\n",
    "tr_b, tr_s, tr_f = sel_dm_nwp_train.shape[0], sel_dm_nwp_train.shape[1], sel_dm_nwp_train.shape[2]      \n",
    "ts_b, ts_s, ts_f = sel_dm_nwp_test.shape[0], sel_dm_nwp_test.shape[1], sel_dm_nwp_test.shape[2]      \n",
    "\n",
    "# .. get restorator with obs range\n",
    "nwp_scaler = MinMaxScaler()   # copy default true\n",
    "obs_scaler = MinMaxScaler()\n",
    "nwp_scaler.fit(sel_dm_nwp_train.view().reshape(tr_b*tr_s, tr_f))\n",
    "obs_scaler.fit(dm_obs_train.view().reshape(tr_b*tr_s, output_size))\n",
    "\n",
    "# .. feature normalize   ( train seq, feature = test seq, feature )\n",
    "nor_dm_nwp_train = nwp_scaler.transform(sel_dm_nwp_train.reshape(tr_b*tr_s, tr_f))\n",
    "nor_dm_nwp_train = nor_dm_nwp_train.reshape(tr_b,tr_s,tr_f)\n",
    "nor_dm_obs_train = obs_scaler.transform(dm_obs_train.reshape(tr_b*tr_s, output_size))\n",
    "nor_dm_obs_train = nor_dm_obs_train.reshape(tr_b,tr_s, output_size)\n",
    "\n",
    "nor_dm_nwp_test = nwp_scaler.transform(sel_dm_nwp_test.reshape(ts_b*ts_s, ts_f))\n",
    "nor_dm_nwp_test = nor_dm_nwp_test.reshape(ts_b,ts_s,ts_f)\n",
    "nor_dm_obs_test = obs_scaler.transform(dm_obs_test.reshape(ts_b*ts_s, output_size))\n",
    "nor_dm_obs_test = nor_dm_obs_test.reshape(ts_b,ts_s, output_size)\n",
    "\n",
    "nor_dm_nwp_train = nor_dm_nwp_train[:,1::,:]\n",
    "nor_dm_obs_train = nor_dm_obs_train[:,1::,:]\n",
    "\n",
    "nor_dm_nwp_test = nor_dm_nwp_test[:,1::,:]\n",
    "nor_dm_obs_test = nor_dm_obs_test[:,1::,:]\n",
    "\n",
    "print ('---------- Final training data shape')\n",
    "print(type(nor_dm_nwp_train))\n",
    "print ('tran nwp : ', nor_dm_nwp_train.shape)\n",
    "print ('tran obs : ', nor_dm_obs_train.shape)\n",
    "print ('test nwp : ', nor_dm_nwp_test.shape)\n",
    "print ('test obs : ', nor_dm_obs_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "from tensorflow.keras import Input, Model, callbacks\n",
    "from tensorflow.keras.utils import plot_model as plm\n",
    "from tensorflow.keras.activations import swish\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tcn import TCN, tcn_full_summary\n",
    "\n",
    "#from skopt import BayesSearchCV\n",
    "#from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "\n",
    "\n",
    "#.. local\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from kmk_make_scorer import r2_3dim, mse_3dim, mae_3dim\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set configure\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Data set\n",
    "\n",
    "element = 'ALLV'\n",
    "name_list = \"./SHEL/namelist.input\"\n",
    "\n",
    "hp_lr = 0.009\n",
    "hp_pd = 'same'\n",
    "hp_ns = 1\n",
    "hp_dl = [1,2,4,8,16,32,48]\n",
    "hp_ldl = hp_dl[-1] # last dilation factor to make name of save model\n",
    "hp_bn = True\n",
    "hp_nf = 80\n",
    "hp_dr = 0.07\n",
    "hp_ks = 6\n",
    "\n",
    "input_size = 6\n",
    "output_size = 2\n",
    "num_fct = 48\n",
    "batch_size = 8\n",
    "n_iter_search = 20\n",
    "num_epoch = 600\n",
    "dev_stn_id = 47105\n",
    "tran_data_per = \"2101_2104_2201_2204\"\n",
    "\n",
    "exp_name = \"CNTL\"\n",
    "csv_outdir = './DAOU/LOSS/' + exp_name + '/'\n",
    "model_outdir = './DAOU/MODL/' + exp_name + '/'\n",
    "scalr_outdir = './DAOU/SCAL/' + exp_name + '/'\n",
    "gifd_outdir = './GIFD/' + exp_name + '/'\n",
    "log_outdir = './DAOU/LOGF/' + exp_name + '/'\n",
    "\n",
    "\n",
    "if os.path.exists(csv_outdir) != True: os.makedirs(csv_outdir)\n",
    "if os.path.exists(model_outdir) != True: os.makedirs(model_outdir)\n",
    "if os.path.exists(scalr_outdir) != True: os.makedirs(scalr_outdir)\n",
    "if os.path.exists(gifd_outdir) != True: os.makedirs(gifd_outdir)\n",
    "if os.path.exists(log_outdir) != True: os.makedirs(log_outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size:  6\n",
      "batch_size:  8\n",
      "time_lenght:  48\n"
     ]
    }
   ],
   "source": [
    "#=========================================================================\n",
    "# .. Model configuration\n",
    "num_cv = 5\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set batch for cross-validation\n",
    "\n",
    "print ('input_size: ', input_size)\n",
    "print ('batch_size: ', batch_size)\n",
    "print ('time_lenght: ', num_fct)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set Model\n",
    "\n",
    "\n",
    "# .. Define model\n",
    "def create_model(dropout_rate=0.15, nb_filters=7, kernel_size=3): \n",
    "\n",
    "          print ('================== Model called ========================')\n",
    "          print ('input_size: ', input_size)\n",
    "          print ('batch_size: ', batch_size)\n",
    "          print ('time_lenght: ', num_fct)\n",
    "          print ('nb_filters: ', nb_filters)\n",
    "          print ('kernel_size: ', kernel_size)\n",
    "          print ('dropout_rate: ', dropout_rate)\n",
    "          print ('dilations: ', hp_dl)\n",
    "          dropout_rate = np.round(dropout_rate,2)\n",
    "          print ('dropout_rate: ', dropout_rate)\n",
    "          \n",
    "          ## .. clear keras model\n",
    "          K.clear_session()\n",
    "\n",
    "          # .. create model\n",
    "          #i = Input( batch_shape=(batch_size, num_fct, input_size) )\n",
    "          i = Input( batch_shape=(None, num_fct, input_size) )\n",
    "          o = TCN(return_sequences=True,\n",
    "                  activation=swish,\n",
    "                  nb_filters=nb_filters,\n",
    "                  padding=hp_pd,\n",
    "                  use_batch_norm = hp_bn,\n",
    "                  nb_stacks=hp_ns,\n",
    "                  dropout_rate=dropout_rate,\n",
    "                  kernel_size=kernel_size,\n",
    "                  use_skip_connections=True,\n",
    "                  dilations=hp_dl\n",
    "                  )(i)\n",
    "          o = TimeDistributed(Dense(output_size, activation='linear'))(o)\n",
    "\n",
    "          # .. compile\n",
    "          adam = optimizers.Adam(lr=hp_lr)\n",
    "\n",
    "          m= Model(inputs=[i], outputs=[o])\n",
    "          m.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "          m.summary()\n",
    "\n",
    "          return m\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': None,\n",
       " 'build_fn': <function __main__.create_model(dropout_rate=0.15, nb_filters=7, kernel_size=3)>,\n",
       " 'warm_start': False,\n",
       " 'random_state': None,\n",
       " 'optimizer': 'rmsprop',\n",
       " 'loss': None,\n",
       " 'metrics': None,\n",
       " 'batch_size': 8,\n",
       " 'validation_batch_size': None,\n",
       " 'verbose': 1,\n",
       " 'callbacks': None,\n",
       " 'validation_split': 0.0,\n",
       " 'shuffle': True,\n",
       " 'run_eagerly': False,\n",
       " 'epochs': 600}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = create_model()\n",
    "#model.count_params\n",
    "\n",
    "# .. Wrapping create_model for searching hyper-parameter\n",
    "model = KerasRegressor(build_fn=create_model,\n",
    "                       verbose=1,\n",
    "                       epochs=num_epoch,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_filters': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x00000204FF452250>}\n",
      "(108, 48, 6) (108, 48, 2)\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter nb_filters for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(nb_filters=47)`\nCheck the list of available parameters with `estimator.get_params().keys()`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"c:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\parallel.py\", line 588, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\parallel.py\", line 588, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"c:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 720, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"c:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\scikeras\\wrappers.py\", line 1168, in set_params\n    raise ValueError(\nValueError: Invalid parameter nb_filters for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(nb_filters=47)`\nCheck the list of available parameters with `estimator.get_params().keys()`\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m start \u001b[39m=\u001b[39m time()\n\u001b[0;32m     48\u001b[0m \u001b[39mprint\u001b[39m(nor_dm_nwp_train\u001b[39m.\u001b[39mshape, nor_dm_obs_train\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 49\u001b[0m optimizer\u001b[39m.\u001b[39;49mfit(nor_dm_nwp_train, nor_dm_obs_train)\n\u001b[0;32m     51\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(optimizer\u001b[39m.\u001b[39mcv_results_))\n\u001b[0;32m     52\u001b[0m \u001b[39mprint\u001b[39m(optimizer\u001b[39m.\u001b[39mcv_results_)\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1805\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1806\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1807\u001b[0m         ParameterSampler(\n\u001b[0;32m   1808\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1809\u001b[0m         )\n\u001b[0;32m   1810\u001b[0m     )\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\parallel.py:1691\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1684\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1685\u001b[0m \n\u001b[0;32m   1686\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[1;32m-> 1691\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_error_fast()\n\u001b[0;32m   1692\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\parallel.py:1726\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[39m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1726\u001b[0m     error_job\u001b[39m.\u001b[39;49mget_result(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout)\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\parallel.py:735\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    729\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[0;32m    731\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    732\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[1;32m--> 735\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_return_or_raise()\n\u001b[0;32m    737\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\workspace\\VScode_project\\prj_mountain2\\.venv\\lib\\site-packages\\joblib\\parallel.py:753\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    752\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 753\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter nb_filters for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(nb_filters=47)`\nCheck the list of available parameters with `estimator.get_params().keys()`"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Use bayes_opt\n",
    "\n",
    "# .. Exp para set\n",
    "#param_dist = { 'padding': Categorical(['causal','same']),\n",
    "#               'nb_stacks': Integer(1,5),\n",
    "#               'nb_filters': Categorical([7,20,30]),\n",
    "#               'kernel_size': Integer(2,24) }\n",
    "#param_dist = { 'dropout_rate': Real(0.01, 0.2),\n",
    "#               'nb_filters': Integer(50,100),\n",
    "#               'kernel_size': Integer(3,12) }\n",
    "##param_dist = { 'dropout_rate': Categorical([0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.15]),\n",
    "#               'nb_filters': Integer(50,100),\n",
    "#               'kernel_size': Integer(3,12) }\n",
    "param_dist = { 'nb_filters': randint(10, 100)}\n",
    "\n",
    "#               'kernel_size': [3,6,9,12] }\n",
    "\n",
    "set_eval_score = { 'MAE': make_scorer(mae_3dim),\n",
    "                   'MSE': make_scorer(mse_3dim),\n",
    "                   'R2': make_scorer(r2_3dim) }\n",
    "\n",
    "print ( param_dist )\n",
    "\n",
    "# optimizer =  BayesSearchCV( estimator=model,\n",
    "#                             search_spaces=param_dist,\n",
    "#                             scoring=make_scorer(r2_3dim),\n",
    "#                             refit=False,\n",
    "#                             cv=num_cv,\n",
    "#                             n_iter=n_iter_search,\n",
    "#                             return_train_score=True,\n",
    "#                             verbose=1,\n",
    "#                             n_jobs=1,\n",
    "#                             random_state=1 )\n",
    "\n",
    "optimizer =  RandomizedSearchCV( estimator=model,\n",
    "                            param_distributions=param_dist,\n",
    "                            scoring=make_scorer(r2_3dim),\n",
    "                            refit=False,\n",
    "                            cv=num_cv,\n",
    "                            n_iter=n_iter_search,\n",
    "                            return_train_score=True,\n",
    "                            verbose=1,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=1 )\n",
    "\n",
    "start = time()\n",
    "print(nor_dm_nwp_train.shape, nor_dm_obs_train.shape)\n",
    "optimizer.fit(nor_dm_nwp_train, nor_dm_obs_train)\n",
    "\n",
    "print(type(optimizer.cv_results_))\n",
    "print(optimizer.cv_results_)\n",
    "\n",
    "# .. Report\n",
    "def report(result, n_top=n_iter_search):\n",
    "    for i in range(n_top):\n",
    "        candidates = [ result['rank_test_score'][i] ]\n",
    "        for candidate in candidates:\n",
    "            print(\"Rank: %0d, R2: %.3f with %r\" %\n",
    "                  ( i, result['mean_test_score'][candidate-1],\n",
    "                       result['params'][candidate-1] ) )\n",
    "\n",
    "\n",
    "print(\"BayesSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings. \" % ((time() - start), n_iter_search))\n",
    "\n",
    "print( \"Best: %f using %s\" % ( optimizer.best_score_,\n",
    "                               optimizer.best_params_ ) )\n",
    "\n",
    "report(optimizer.cv_results_)\n",
    "\n",
    "\n",
    "#=========================================================================\n",
    "# .. Second refit to evaluate best model use test period\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# .. Set model label\n",
    "\n",
    "# .. best model configuration for whole train set\n",
    "params = optimizer.best_params_\n",
    "print(type(params))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  6,  9, 12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(3,15,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
